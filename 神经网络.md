# 神经网络

标签（空格分隔）： 机器学习

---
**神经网络的学习过程就是：
根据训练数据来调整神经元之间的“连接权”以及每个功能神经元的阈值。**

# 神经元模型
![在此输入正文][1]


  神经网络中最基本的成分是神经元模型，把许多个这样的神经元按一定的层次结构连接起来，就得到了神经网络。
  
 其他神经元的输入x，经过连接权重w，作用于该神经元上。其实神经网络学习过程，就是根据训练数据来调整神经元之间的“连接权”，以及每个功能神经元的阈值。


# 感知机与多层网络
从感知机进一步理解神经网络。

感知机（Perceptron）由两层神经元组成（输入层、输出层），输入层接收外界输入信号后传递给输出层，输出层是M-P神经元，亦称“阈值逻辑单元”（threshold logic unit）。
 
`特点`：两层神经元，意味着感知机只有一层输出层是功能神经元（进行激活函数的处理），感知机能容易的实现逻辑与、或、非运算，因为这些都是线性可分的（即可以用一个超平面分开），但对于非线性可分的问题就无法解决了，所以学习能力非常有限。
所以，要解决非线性可分的问题，就需要使用多层功能神经元，这时候，隐层和输出层都是具有激活函数的功能神经元。

![此处输入图片的描述][2]


##多层前馈神经网络（multi-layer feedforward neural networks）
每层神经元与下一层神经元完全互连，神经元之间不存在同层连接，也不存在跨层连接。

“前馈”：网络拓扑结构上不存在环或回路。
![此处输入图片的描述][3]


#误差逆传播（error BackPropagation，简称BP）算法
多层前馈神经网络，这样的网络学习能力比感知机要强很多，训练也需要更强大的学习算法，一个典型算法是`BP算法`。

（也可以训练其他类型神经网络，一般是多层前馈神经网络）
![此处输入图片的描述][4]


## 步长
BP是一个迭代算法，每一步迭代采用广义感知机学习规则对参数进行更新估计。

![此处输入图片的描述][5]


这样的每次更新，只针对单个样例去更新连接权和阈值，参数更新非常频繁，也有可能不同样例的参数更新的效果可能出现“抵消”，所以提出“累积BP算法”，读取所有训练集之后才进行一次参数更新，所以，参数更新频率小很多。

但是，累积BP算法在累积误差下降到一定程度的时候，很难再下降，非常缓慢，所以，标准BP算法往往会更快获得较好的解，尤其在训练集D非常大的时候更明显。

## 过拟合
BP强大的表示能力使得BP神经网络经常遭遇过拟合，为了避免过拟合，有两种方式：

1. 早停
将数据分成训练集和验证集，训练集用来计算梯度、更新连接权和阈值，验证集用来估计误差，若训练集误差降低但验证集误差升高，则停止训练，同事返回具有最小验证集误差的连接权和阈值。

2. 正则化


# 全局最小与局部极小

前面说过：**神经网络的学习过程就是：
根据训练数据来调整神经元之间的“连接权”以及每个功能神经元的阈值。**

***E*** 是神经网络在训练集上的误差，很显然，***E*** 是关于连接权和阈值的函数。而神经网络学习的目的就是使得***E***最小。
换句话说，神经网络的训练过程可看作一个参数寻优的过程，找到一组最优参数使得***E***最小。

## 梯度下降

基于梯度的搜索是使用最为广泛的参数寻优方法，为了防止陷入局部极小值，采用以下方法“跳出”局部极小值。

1. 不同参数初始化不同神经网络，陷入不同局部极小值，以进行选择最接近全局最小值的结果。
2. 模拟退火
3. 随机梯度下降









  [1]: http://images2015.cnblogs.com/blog/1007623/201707/1007623-20170725171252654-1064098828.png
  [2]: http://images2015.cnblogs.com/blog/1007623/201707/1007623-20170725171257404-432992394.png
  [3]: http://images2015.cnblogs.com/blog/1007623/201707/1007623-20170725171259201-1851098378.png
  [4]: http://images2015.cnblogs.com/blog/1007623/201707/1007623-20170725171259654-1114411322.png
  [5]: http://images2015.cnblogs.com/blog/1007623/201707/1007623-20170725171305935-1361097696.png