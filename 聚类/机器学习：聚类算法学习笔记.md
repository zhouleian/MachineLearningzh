# 机器学习：聚类算法学习笔记

标签（空格分隔）： 机器学习

---

聚类是机器学习中新算法出现最快，最多的算法，原因一是聚类不存在客观标准，二是“给定数据集，总能从某个角度找到以往算法，未覆盖的某种标准从而设计出新算法。
k-means聚类之前介绍过了，这里简单介绍其他聚类算法的概念。

#一、原型聚类
## 1.1 k-means算法
另一篇博客
## 1.2 学习向量量化 LVQ
LVQ算法假设训练数据有类别标签，然后利用假设的类别来辅助聚类

以二分类为例，

1. 假设训练数据的分类c1，c2；这样输入数据$D=\lbrace{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)}\rbrace $
其中，$y_1,...,y_n$ 为 $ c_1 或 c_2$。并假定原型向量个数 $q$,即聚类簇数,并假定 $q_i$ 对应的类别标记。

2.  while ：随机选择一个样本点，计算该样本数据点和 $q_i$ 的距离，选择最近的 $q_i$。

3.  对原型向量进行更新：
(1) 如果事先假定的样本点的类别 $y_j$ 和 $q_i$一致，则根据公式使得原型向量 $q_i$ 在更新后靠近 $(x_j,y_j)$ 点。
(2) 如果事先假定的样本点的类别 $y_j$ 和 $q_i$ 不一致，则根据公式使得原型向量 $q_i$ 在更新后远离 $(x_j,y_j)$ 点。

4. 当迭代轮数达到最大，或者原型向量的更新很小或不变的时候，while结束。


# 二、密度聚类

> 输入：样本集 $D=\lbrace{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)}\rbrace $，
    邻域参数(ϵ,MinPts)
    样本距离度量方式
    
1. 找到样本集中所有核心对象。

> 输出： 簇划分C.　
1）初始化核心对象集合Ω=∅, 初始化聚类簇数k=0，
    初始化未访问样本集合 Γ,Γ = D,  
    簇划分C = ∅
$ $
2) $for$  $j=1,2,...m$, 按下面的步骤找出所有的核心对象：
    a) 通过距离度量方式，找到样本 $x_j$ 的ϵ-邻域子样本集$N_ϵ(x_j)$
    b) 如果子样本集样本个数满足 |$N_ϵ(x_j)$| ≥ MinPts,将样本
$x_j$ 加入核心对象样本集合：Ω=Ω∪{$x_j$}

$ $

>3）如果核心对象集合Ω=∅，则算法结束，否则转入步骤4.

2. 这时核心结合对象集合中不为空，从核心对象集合中随机选择一个作为“种子”（第一个随机选择的叫种子），由此出发确定相应的聚簇类。根据种子，不断循环，得到该种子核心对象邻域中的所有样本，如果在这些样本中包含了其他核心对象i，进一步继续得到i邻域中的样本。 由此得到第一个类簇 c1。

>4）在核心对象集合Ω中，随机选择一个核心对象o，初始化当前簇核心对象队列 Q = {o}, 初始化类别序号k=k+1，初始化当前簇样本集合
更新未访问样本集合Γ=Γ−{o}

>5）如果当前簇核心对象队列Ωcur=∅，则当前聚类簇Ck
生成完毕, 更新簇划分C={C1,C2,...,Ck}, 更新核心对象集合
Ω=Ω−Ck， 转入步骤3。

>6）在当前簇核心对象队列Ωcur中取出一个核心对象o′,通过邻域距离阈值ϵ找出所有的ϵ-邻域子样本集Nϵ(o′)，令Δ=Nϵ(o′)∩Γ, 更新当前簇样本集合Ck=Ck∪Δ, 更新未访问样本集合Γ=Γ−Δ,转入步骤5.
　
>输出结果为： 簇划分 C={$c_1$,...,$c_k$}

`优点：`
1. 不需要输入类别数k；
2. 最大的优势是可以发现任意形状的稠密数据集进行聚类，而不是像K-Means，一般仅仅使用于凸数据集；
3. 在聚类时，可以找到数据集中的异常点；
4. 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。
所以，一般来说，如果数据集是稠密的，并且数据集不是凸的，那么用DBSCAN会比K-Means聚类效果好很多。如果数据集不是稠密的，则不推荐用DBSCAN来聚类；

`缺点`：
1. 如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。
2. 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。
3. 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。
　　　　

### 三、 层次聚类