# 机器学习：Boosting学习笔记

标签（空格分隔）： 机器学习

---

# 一、应用背景

前面介绍了集成学习，集成学习方法按照个体学习器之间是否存在依赖关系可以分为两类：
1. 第一个是个体学习器之间存在强依赖关系，必须串行执生成的序列化方法：代表算法是Boosting算法； 在本篇介绍
2. 第二个是个体学习器之间不存在强依赖关系，可同时生成的并行化方法，代表算法是bagging和随机森林（Random Forest）算法。

目前，Boosting算法主要有：AdaBoost、提升树，现依次介绍具体实现。


# 二、 AdaBoost算法基本概念

AdaBoost是一类将多个弱学习器提升为强学习器的机器学习算法，是Boosting算法中最著名的代表。

前面介绍集成学习博客中提过，集成学习的关键：
 1. 如何产生每个分类器？在每一轮如何改变训练数据的权值或概率分布；
 2. 如何结合各个弱学习器组合得到强分类器？

下面介绍AdaBoost算法，解释AdaBoost是如何解决这两个关键问题的，并解释其效果不错的原因。
 
## 2.1 算法介绍

![AdaBoost][1]

1. 首先，为训练数据中的每个样本赋予相同的初始权重，这些权重构成向量D；

2. 从初始训练集用初始权重训练出一个弱学习器，计算该分类器的学习误差 ϵ， 根据该弱学习器的误差率的表现来调整训练样本分布：
其中，使之前弱学习器学习误差率高的训练样本点的权重变高，分对的样本的权重变低，这样在后面，先前弱学习器做错的训练样本载后面的学习器中得到更多的关注；**（这解决了集成学习的第一个关键）**
每个分类器都分配了一个 α ，是根据错误率 ϵ 计算得到的，作用在最后的强分类器上。

 $$ ϵ  = \frac{未分类正确的样本数}{总样本数} $$
 这个 ϵ 不同于其他的AdaBoost算法中的误差率计算，需要其他公式来计算误差率。
 
 $$ α = \frac{1}{2}ln(\frac{1-ϵ}{ϵ}) $$
 
3. 然后基于调整权重后的训练集来训练下一个弱学习器；

4. 如此重复进行，直到训练错误率为0，或者，弱学习器数达到事先指定的数目T，将这T个弱学习器进行加权结合，得到最终的强学习器。

**`加权结合`** 解决了集成学习的第二个关键问题，具体的说：
加权多数表决机制，即加大误差率小的弱分类器的权值，使其在表决中获得较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。（一句话：误差率越小，最后的表决起的作用越大。）

## 2.1 算法流程
给定二分类训练集，其中 
$m=1,2, … , M,$ 为迭代的轮次. 
$ D_m =(w_{mi},...,w_{mn}$, 记为第k轮样本的权值分布. 相应地
$ w_{mi}$表示第i个样本在第m轮的权重. 
$G_m(x) $为第m次迭代学习到的基学习器. 
$e_m$为$ G_m(x)$ 在训练集上的表现误差. 
$α_m$为Gm的系数.

> 输入：
训练数据集 T={(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>),…,(x<sub>N</sub>,y<sub>N</sub>))} 
其中，x<sub>i</sub>∈X⊆R<sub>N</sub>为实例的特征向量，yi∈Y={1，-1}为实例的类别，i=1,2,…,N ;；

$ $
> 输出：
最终分类器 G(x)

过程：

1. 初始化训练数据的权重分布
$ D_1 = (W_{11},...,w_{1N}), w_{1i} = \frac{1}{N}$
$i = 1,2,...,N$

2. 对 $m=1,2,...，M$

(a) 使用具有权值分布 $D_m$的训练数据集学习，得到基本分类器，
$ G_m(x) : \chi -> \lbrace1,-1 \rbrace$

(b) 计算$G_m(x)$在训练数据集上的分类误差率
$$ e_m = P(G_m(x_i)≠y_i) = \sum^N_{i=1} w_{mi}I(G_m(x_i)≠y_i)$$

(c) 计算$G_m(x)$的系数
$$ \alpha_m = \frac{1}{2}log{\frac{1-e_m}{e_m}} $$

(d) 更新权值
![此处输入图片的描述][2]
其中$Z_m$为规范化因子

(3) 构建线性组合的强分类器
$$ f(x) = \sum^M_{m=1}\alpha_mG_m(x) $$

第四部分，我们看一个实例，直观执行一次算法。

## 3.2 算法步骤 

AdaBoost算法的基本流程是：


1. `收集数据`：采用任意方法收集
2. `准备数据`：依赖于所使用的分类器类型

3. `分析数据`：采用任意方法对数据进行分析
4. `训练算法`：AdaBoost大部分时间将用于训练，并且是多次在同一个数据集上训练弱分类器

5. `测试算法`：计算分类错误率。
6. `使用算法`：预测类别。



## 3.3 算法优缺点
`优点`：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整

`缺点`：对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。

`适用数据类型`：数值型和标称型数据（即分类离散值）

## 3.4 适用场景
AdaBoost算法通过多个弱分类器的组合得到强分类器，实现简单；并且，分类错误率随着训练增加而下降，不会过拟合，非常适合于各种分类场景。





# 四、实例
下面以表格中的训练数据，使用AdaBoost算法训练一个强分类器。
\begin{array}{c|lcr}
序号 & 1 & 2 & 3 &4&5&6&7&8&9&10\\
\hline
x & 0& 1 & 2&3&4&5&6&7&8&9\\
y & 1 & 1 & 1&-1&-1&-1&1&1&1&-1 \\
\end{array}

## 4.1 解答
1. 初始化权值分布：
$$ D_1 = (W_{11},...,w_{110}), w_{1i} = 0.1$$
$$i = 1,2,...,N$$

2. 对 $m=1$
(a) 在权值分布为$D_1$的训练数据集上，阈值v取值为2.5的时候分类误差率最低，故基学习器为
$$ G_1(x)= \begin{cases} 1, & \text {x<2.5} \\ -1, & \text{x>2.5} \end{cases} $$
(b) 计算误差率
在训练数据上有3个误分类点
$$ e_m = P(G_m(x_i)≠y_i) = \sum^N_{i=1} w_{mi}I(G_m(x_i)≠y_i)$$
所以，$$ e_m = 0.1*3 = 0.3 $$
(c)依据公式计算 $\alpha = 0.4236$
(d)更新权重分布
$$ D_2 = (W_{11},...,w_{110}), w_{1i} = 0.1$$
$$w_im$$
根据上面提到的计算公式计算：$D_2$

$f_1(x) = 0.4236G_1(x)$

之后，依次计算$m=2,3...$的时候，当 $m=3$时，计算得到训练数据集上的误分类点个数为0.此时训练结束，最终的强分类器为：

$G(x) = sign[f_3(x)] $
$$ =sign[0.4236G_1(x) + 0.6496G_2(x) + 0.7514G_3(x)] $$

  [1]: http://omxy7x542.bkt.clouddn.com/17-12-18/8460091.jpg
  [2]: http://omxy7x542.bkt.clouddn.com/17-12-18/17805244.jpg