# 机器学习：最大熵模型学习笔记

标签（空格分隔）： 机器学习

---


# 一、应用背景

在统计学习方法三要素（模型，策略，算法）中，模型包括模型评估（训练误差，测试误差）和模型选择，模型选择有多种方法，之前介绍过正则化，交叉验证，这里介绍最大熵原理，最大熵原理是机器学习中选择最优模型的方法，将它应用到分类任务中就是最大熵模型。


所以，最大熵模型的提出是以最大熵理论为基础的。下面一一介绍，什么是熵，熵是怎么提出的，信息学中熵指什么，什么是最大熵，机器学习中的最大熵模型是什么，最大熵如何概率模型联系等。



# 二、 最大熵模型的基本概念

第一次接触到“熵”是机器学习西瓜书中的决策树部分，决策树的特征选择部分，要选择“纯度”最高的特征需要计算“信息熵”。但第一次“熵”的使用是在热力学中，用来表示系统的混乱程度。


## 2.1 热力学熵

为什么“熵”可以表示系统的混乱程度？首先，热与温度的商 为“熵”，下面介绍“热”和温度之间的关系，以解释“熵”为什么可以表示系统的混乱程度。

### 2.1.1 热
1827年，英国植物学家布朗通过研究花粉在水面上不停的运动，且运动轨迹极不规则。经过试验证明这种运动的原因不是因为震动或液体对流这样的外界影响，而是在液体内部。原来花粉在水面运动是受到各个方向水分子的撞击引起的。于是这种运动叫做布朗运动，表明液体分子在不停地做无规则运动。并且实验观察到，随着温度的升高布朗运动愈加剧烈。这表示分子的无规则运动跟温度有关系，温度越高，分子的无规则运动就越激烈。正因为分子的无规则运动与温度有关系，所以通常把分子的这种运动叫做分子的热运动。

所以，热（heat）是大量分子无规则运动的一种表现，在一定程度上表示了系统混乱度。

### 2.1.2 热与做功

考虑水车，水从高处流往低处，水量并没有变化，但是在流的过程做了功。
热力学中，热力学能与动能、势能一样，是物体的一个状态量。
能量守恒定律宣称，宇宙中的能量永远保持相同的值。那么，能够把能量无止境地转化为功吗？既然能量不灭，那么它是否可以一次又一次地转变为功？

1824年，法国物理学家卡诺把热量想象成像水一样的物质，并证明：热力学系统做功的能力，不取决于热量本身，而取决于冷热之间的温差。
即如果做功，那么系统中的热能必须是不均匀分布的，在做功的同时，不同部分之间热能的差异逐渐减小。当能量均匀分布时，就不能再做功了，尽管此时所有的能量依然还存在着。

比如，把一块热石头投入冷水，产生的蒸汽可以做功，但石头和水组成的系统的总热量维持不变，并且最终石头和水会逐渐达到同一温度（熵最大状态）

所以，无论一个封闭系统内有多少能量，只要系统内的所有物体的温度都相同，它就无法对外做功。

### 2.1.3 热力学熵的提出

1850年，德国物理学家鲁道夫克劳修斯通过综合能量守恒、转化定律和卡诺原理，并根据“热传导总是从高温到低温而不能反过来“这一事实，在1850年的论文中提出：一个自动运作的机器，不可能把热量从低温物体传到高温物体而不引起其他变化。这就是热力学第二定律。（能量守恒则是热力学第一定律）

考虑，高温物体和低温物体接触时，高温物体会自发的将热能传递给低温物体，最终两个物体温度相等。这一过程中，热量转换为功的时候，热量并未损失，只是从较热的物体传递到了较冷的物质。但是，相反的过程不会自发的发生。宇宙中这样的过程有很多，这些过程的发生是具有一定方向性的，即为“不可逆过程”。很显然，不可逆过程的前后状态是不等价的。那么怎么衡量这种“不等价性”？

1854年，鲁道夫克劳修斯通过研究这种不可逆过程中，找到了热和温度之间的某种关系，并证明在能量差异逐渐变小的同时，这种关系在数值上总是增加。并在1865年首次引入了$\color{blue}{熵}$这个概念，源自希腊语的 “转换”，来定义发现的这种关系。
总而言之，热力学第二定律说明了热和功之间的转换，功是一种有方向的行为，（只能从高温到低温），热则是无需的。并指出，自发的过程都是不可逆过程，是一种从有序到无序的过程。
### 2.1.4 热力学第二定律

克劳修斯提出熵之后，并将热力学第二定律表述为：在孤立系统中，实际发生的过程总是朝着热力学平衡状态演化，使整个系统的熵增加。

在这之后，克劳修斯把孤立体系中的熵增定律扩展到了整个宇宙中，宇宙的熵衡增。认为在整个宇宙中热量不断地从高温转向低温，直至一个时刻不再有温差，宇宙总熵值达到极大。这时将不再会有任何力量能够使热量发生转移，即“热寂论”。



同时，	热力学第二定律是大量分子无规则运动的统计规律，不是由于某种物理原因造成的（花粉运动是水分子运动的结果，不是外界震动造成的），但是，在分子水平，这条定律会被随机的违背。所以只适用于大量分子构成的系统，对于单个分子构成的系统不适用。

### 2.1.5 熵增加原理
	
前面说过，一个**`孤立系统`**，熵不减少，总是增大或者不变。熵在可逆过程中不变，在不可逆过程中增加。

这里的可逆和不可逆都是统计意义上的，不可逆性是由于统计上可逆发生的概率极小，从而，在统计上，万事万物都将趋于熵最大化。万事万物的微观状态的数量（宏观的可能性）都在增加。

那么，这样的热力学第二定律，熵增原理是否一直如此？为了反驳热力学第二定律，麦克斯韦认为存在一种和熵增原理相抗衡的机制，他不知道如何描述这种机制，幽默的称为“妖”，即“麦克斯韦妖”。


### 2.1.5 无序与有序

前面说过，自然界中这样的热和功之间的有序和无序的特性，怎么和机器学习中的概率相关联？

计算一个系统的所有可能组合，其中无序的状态要远远多于有序的状态。在大多数组合或者“状态”中，分子通常都是乱作一团的，只有在少数状态中，分子是整齐有序的。

因此，有序状态的熵低，出现的概率也低；而如果要达到客观的有序度，其出现的概率会非常低。

**$\color{blue}{这一段，需要问问师哥,并不太理解}$**

我理解：在自然界中也是这样的一种情况，有序状态出现的概率低，熵低。即越无序，熵越大；能量越不可用，熵越大

### 2.1.5 玻尔兹曼关系（玻尔兹曼熵）
玻尔兹曼关系是对熵的微观（**统计意义的**）解释，表述为：系统的熵S与其微观状态数W存在函数关系
$$S = k lnW$$
其中k为玻尔兹曼常数,可通过热力学第一定律，熵的热力学定义，及麦克斯韦-玻尔兹曼统计推出>

玻尔兹曼关系给出了熵的微观解释——系统微观粒子的无序程度的度量，并对熵这一概念引入信息论、生态学等其他领域具有深远意义。
	
## 2.2 信息熵

热力学中用熵来表示分子状态的混乱程度，有序状态的熵低，出现的概率也低；而如果要达到客观的有序度，其出现的概率会非常低。由此，熵和概率有了联系。

香农在信息论中用熵表示的对随机变量不确定度的度量。在决策树中，用信息熵计算纯度，选择特征，信息熵最根本的意义是度量信息的多少，这个变量的信息熵越大，说明这个变量包含的样本越多，纯度越低。
要找纯度越高的，即找信息熵越小的。样本集合D的信息熵为：

$ Ent(D) =  - \sum_{k=1}^{|y|} p_klog_2p_k$

其中， $p_k$为当前样本集合D中第 $k$类样本所占的比例。这是统计学上的意义，和玻尔兹曼公式异曲同工。考虑投掷硬币正反面的例子，投掷3枚硬币，描述一个系统所需的信息量（信息熵）与观测的精细程度相关，比如，可以分清正反面和不会分正反面两种情况，这个精细度存在物理极限，达到物理极限时，玻尔兹曼熵就是信息熵，系统的物理状态数就是这个系统包含的/携带的信息量的上限。

热力学熵描述的是“混乱度”，但这种混乱度只是“不混乱程度”的一种描述，热力学研究者真正关心的也是“不混乱程度”，即热缩包含的能量可用于做功的潜力大小。这是熵的真正的用途。所以，“信息熵”描述了信息的有序程度，以及这种有序蕴含的价值。



# 三、最大熵模型
在热力学中，熵最大的时候就是系统保持平衡的时候，同理，在模型选择中，最好的模型就是熵最大的模型。

下面介绍最大熵原理，最大熵模型。

## 3.1 熵

最大熵中的熵即信息熵。样本集合D的信息熵为：

$ Ent(D) =  - \sum_{k=1}^{|y|} p_klog_2p_k$

其中， $p_k$为当前样本集合D中第 $k$类样本所占的比例。

## 3.2 最大熵原理

最大熵原理：学习概率模型时，在所有可能的概率模型中，熵最大的模型就是最好的模型。
例如，对于熵的公式，
&$ H(D) =  - \sum_{k=1}^{|y|} p_klog_2p_k$$

熵满足公式：
$$0 <= H(P) <=log_2p_k$$
当且仅当X的分布是均匀分布时，右侧等号成立，即$\quad\color{blue}{熵最大}$。

下面用一个随机变量的例子直观介绍最大熵模型，最大熵是怎么得到的。

## 3.3 最大熵模型
将最大熵原理用于分类即为最大熵模型，所以最大熵模型

### 3.3.1 两个原则
最大熵模型满足两个原则：
(1) 选择概率模型必须满足已有事实（约束条件），在满足约束条件集合中寻找最优模型；
(2) 对未知的情况不做任何主观假设，保持均衡，这时，未知情况的分布是最均匀的。
在这种情况下，概率分布最均匀，可以保留全部的不确定性（熵最大），把风险降低到最小，因此得到的概率分布的熵是最大。

### 3.3.2 例子
对于随机变量X，其可能的取值为 {A,B,C，D}
，没有任何约束的情况下下，各个值等概率得到的 最大熵 模型为：
$$P(A)=P(B)=P(C)=P(D)=\frac{1}{4}$$

当给定一个约束 ：
$$P(A)=\frac{1}{2}$$
可以得到满足约束条件的模型：
$$P(A)=\frac{1}{2}\quad\quad\quad\quad\quad\quad\quad\quad（1）$$
$$P(A)+P(B)+P(C)+P(D)=1\quad\quad\quad\quad（2）$$
以上(1)(2)两式即是满足约束条件，这时，可以得到无穷多个解，如(A)=0.5，P(B)=0.1，P(C)=0.3，P(D)=0.1或者P(A)=0.2， P(B)=0.1, P(C)=0.2这两个都是上述模型的解。

根据最大熵原理：**1.满足约束条件；2.对未知的均匀分布**
则满足该约束条件下的 MaxEnt 模型是：
$$P(A)=\frac{1}{2} $$
$$P(B) = P(C)=P(D) =\frac{1}{6} $$

这样求解最大熵的过程就是最大熵模型的学习过程，那么对于无法直接平均分布的问题，就需要求解约束转化为最优化问题，常用方法：对偶函数极大化，极大似然估计等.
最优化问题：
 $$ min -H(P) = \sum_{x,y} \tilde{P}(x)P(y|x)logP(y|x)$$
 $$ s.t. \quad \quad P(f_i) - \tilde{P}(f_i) = 0,i=1,2,...,n  $$
$$ \sum_yP(y|x) =1 $$
 求解这个最优化问题的对偶问题或者使用最大似然估计等都能得到最大熵模型。