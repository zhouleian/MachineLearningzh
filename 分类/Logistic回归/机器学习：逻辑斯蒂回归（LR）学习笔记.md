# 机器学习：逻辑斯蒂回归（LR）学习笔记

标签（空格分隔）： 机器学习

---

# 一、应用背景

Logistic回归算法是机器学习中经典的分类方法，和k-近邻和决策树等算法不同的是，这时一种最优化算法，广泛应用在互联网相关的领域，如患病识别，广告计算和推荐系统，以及其他更为广泛的机器学习领域。同时逻辑回归模型（LR模型）也是深度学习的基本组成单元。

考虑预测是否患病的问题，在二分类中，我们可以输出-1,1判断是否患病。现在考虑问题：通过有一组病人的数据，我们需要预测他们在一段时间后患上心脏病的“可能性”。即 $f(x) = P(+1|x)$，取值范围是区间 $[0,1]$。此时。我们可以获取的训练数据却与二值分类完全一样，x 是病人的基本属性，y 是+1(患心脏病)或 -1（没有患心脏病）。输入数据并没有告诉我们有关“概率” 的信息。

在二值分类中，我们通过w*x 得到一个"score"后，通过取符号运算sign 来预测y 是+1 或 -1。而对于当前问题，我们如同能够将这个score 映射到[0,1] 区间，问题似乎就迎刃而解了。这时候，使用逻辑回归模型即可。

后文而重点讨论LR模型的数学基础，算法步骤以及应用场景。



# 二、逻辑回归基本概念

## 2.1 回归
对于已有训练数据，用某个函数对这些点进行拟合，这个拟合的过程就是回归。
逻辑回归进行分类的原理是即为这样拟合的过程，利用训练数据对分类边界线建立回归公式。


## 2.1 线性模型
线性模型试图学得一个通过属性的线性组合来进行预测的函数：

$$ f(\vec{x} = w_1x_1 + w_2x_2 + ... w_nx_n + b), $$

即 $$ f(\vec{x})  =  \vec{w}^T \vec{x} +b ,$$

 $x_i$ 是 $\vec{x}$ 在第i个属性上的取值。

 线性模型简单易于建模，并且具有较好的可解释性，例如：
 
 $$ f_{好瓜}(x) = 0.2* x_{色泽} + 0.5* x_{根蒂} + 0.3* x_{敲声} + 1 $$
 
可解释性很好：可以从色泽、根蒂、敲声综合判断瓜好不好，而且敲声比色泽更重要。


逻辑回归完成的是分类任务，与二值分类不同，逻辑回归要做的是：找到一个单调连续函数将分类任务的真真实标记 y 和线性回归模型中的预测值联系起来。

所以，LR  是在线性回归模型的基础上，使用sigmoid 函数，将线性模型 $w^Tx$的结果压缩到[0,1]之间，使其拥有概率意义。
（为类a的概率是多少，类b的概率是多少......）

## 2.3 sigmoid 函数
逻辑斯蒂回归选择的映射函数是 S 型的 Sigmoid 函数。
![sigmoid][1]

对于给定输入值$\vec{x}$，如果y是样本x作为正例的可能性，那么1 – y就是其反例的可能性。如果y>0.5,则认为结果属于正例，将其划分为1类，否则划分为0类。虽然，这种方法名为“回归”，但是这样却实现了分类，而且，我们还可以得到近似概率的预测，这对很多需要利用概率辅助决策的任务有很大作用。

所以，LR其本质仍然是一个线性模型，实现相对简单。同时，
LR回归属于概率性判别式模型，因为LR不仅可以预测出“类别”，还可以得到近似概率预测,是有概率意义的；LR回归并没有对数据的分布进行建模，也就是说，LR模型并不知道数据的具体分布，而是直接将判别函数，或者说是分类超平面求解了出来。


# 三、 LR算法

LR模型是基于线性回归模型的一类算法，主要用来解决分类问题。LR进行分类的原理是即为一种拟合的过程，利用训练数据对分类边界线建立回归公式，以此建立公式。前面说这是一种最优化算法，是因为，在训练分类器建立公式时，要找到最佳拟合参数集。

下面一一介绍LR算法是什么，如何构建训练器，怎样找到最佳参数（回归系数）即最优化算法。

## 3.1 算法介绍

LR是一种解决二分类的简便算法。训练集中每个数据样本用一个n维特征向量描述n个属性的值，即 X = {x<sub>1</sub>，x<sub>2</sub>，...，x<sub>n</sub>}，共2个类别，分别用c<sub>1</sub>，c<sub>2</sub>表示。训练数据集共N个样本，则

> 输入：
训练数据集 T={(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>),…,(x<sub>N</sub>,y<sub>N</sub>))} 
其中，x<sub>i</sub>∈X⊆R<sub>N</sub>为实例的特征向量，yi∈Y={c1,c2,…,cK} 为实例的类别，i=1,2,…,N ;新输入的实例表示为x；

> 输出：
实例x所属的类y 

> 分类步骤：
训练分类器，找到最佳回归系数，建立回归公式。


## 3.2 算法步骤 

Logistic回归算法的基本流程是：
![流程图][2]

1. `收集数据`：采用任意方法收集
2. `准备数据`：由于需要进行距离计算，因此要求数据类型为`数值型`。另外，结构化数据格式则最佳

3. `分析数据`：采用任意方法对数据进行分析
4. `训练算法`：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数

5. `测试算法`：一旦训练步骤完成，分类将会很快。
6. `使用算法`：首 先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单回归计算，判定它们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。


LR算法学习时，可以应用极大似然估计法估计模型参数。

## 3.3 最优化算法
实际中，解这个优化问题的方法常用梯度下降（上升）和拟牛顿法。下面一一介绍。

### 3.3.1 梯度下降（上升）法


梯度上升算法用来求函数的最大值，梯度下降算法用来求函数的最小值。
要寻找目标函数曲线的波谷，这个方向可以通过微分得到。

梯度下降法的思想是：要找到某个函数的最小值，即要找到目标函数的波谷，采用贪心法：想象一个小人站在半山腰，他朝哪个方向跨一步，可以使他距离谷底更近（位置更低），就朝这个方向前进。这个方向的可以通过微分得到，即最好的方法是沿着函数的梯度方向寻找（函数值下降最快的方向）。

解决了最小值的移动方向问题，移动量也很重要，移动量成为步长（学习率），步长称为$\alpha$。则梯度下降算法的迭代公式为：

$$ w:= w - \alpha\nabla_wf(w) $$

梯度上升算法是：

$$ w:= w + \alpha\nabla_wf(w) $$

这个公式一致迭代执行，直到达到某个停止条件为止。

此时，还涉及$\alpha$的初始值的问题，步子（学习率）太小的话，速度太慢；过大的话，容易发生抖动，可能到不了谷底。

显然，距离谷底较远（位置较高）时，步幅大些比较好；接近谷底时，步幅小些比较好（以免跨过界）。距离谷底的远近可以通过梯度（斜率）的数值大小间接反映，接近谷底时，坡度会减小。所以，我们希望步幅与梯度数值大小正相关。

算法迭代伪代码：

```
//迭代次数为 R
每个回归系数初始化为1
　　　　重复R次：
　　　　　　计算整个数据集的梯度
　　　　　　使用alpha*gradient更新回归系数的向量
　　　　　　返回回归系数
```
对某个数据集进行随机梯度下降算法，迭代200 得到的决策边界图：
![此处输入图片的描述][3]
可知，该决策边界效果较好，但是梯度下降也有着不可避免的缺陷：

每次更新回归系数都要遍历整个数据集，算法复杂度太高，改进：是一次仅用一个样本点来更新回归系数，该方法称为随机梯度上升算法。由于随机梯度下降算法这种每次使用一个样本的特性，使得随机梯度下降算法可以在样本到来时对分类器进行增量式更新，因而是一个$\color{blue}{在线学习算法}$。

## 3.3 随机梯度下降算法

随机梯度上升算法是一次仅用一个样本点来更新回归系数的方法。并且一次迭代，效果不好，可以迭代多次，效果会好很多。

通过对随机梯度下降算法进行多次迭代发现，在大的波动停止后，还有一些小的波动，因为训练集中会存在一些不能正确分类的样本点，每次迭代的时候会造成系数的剧烈改变。所以，我们期望能够避免来回波动，收敛到某个值，并且收敛速度加快。

1. 步长 $\alpha$ 在公式中的变化，有一个常数项, $\alpha$ 不会为0
	保证多次迭代之后新数据仍然有一定影响，$\color{blue}{在线学习算法}$。
	若问题动态变化，则可以适当加大常数项，确保新的值获得更大的回归系数。

2. 随机选取更新样本更新回归系数，减少周期性的波动
	每次随机从列表中选择一个值，然后删除该值


## 3.3 算法优缺点
`优点`：计算代价不高，实现简单，易于建模，具有较好的可解释性。

`缺点`：容易欠拟合，分类精度可能不高

`适用数据类型`：数值型和标称型数据（即分类离散值）

# 四、实例
使用 testSet.txt 数据集中样本，配合sklearn库实现逻辑回归二分类。
```
classifier = LogisticRegression(C=1e6, random_state=0)
classifier.fit(train_feature, train_label)
predictions=classifier.predict(test_feature)

scores = cross_val_score(classifier, train_feature, train_label, cv=5,scoring='accuracy')
    print('准确率', np.mean(scores), scores)
// 97%
```
![此处输入图片的描述][4]
数据集进行了二分类，可以看到效果不错，同时选择了部分样本（test set）测试（图中的紫色），并且得到了97%的准确率，效果不错了。

  [1]: http://omxy7x542.bkt.clouddn.com/17-12-11/91602482.jpg
  [2]: http://omxy7x542.bkt.clouddn.com/17-12-11/11421753.jpg
  [3]: http://omxy7x542.bkt.clouddn.com/17-12-11/87608643.jpg
  [4]: http://omxy7x542.bkt.clouddn.com/17-12-24/64210677.jpg