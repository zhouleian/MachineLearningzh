# 机器学习：k-近邻算法学习笔记

标签（空格分隔）： 机器学习

---

# 一、应用背景

k-近邻算法是机器学习中基本的分类与回归方法，广泛应用在互联网相关的领域，如电影题材分类，手写系统识别，以及其他更为广泛的机器学习领域。



# 二、k-近邻分类算法
k-近邻是一种常用的监督学习方法，主要用来解决分类与回归问题。

分类原理是给定测试样本和训练数据集，训练数据集中的样本类别已定（监督），使用某种距离计算方法，找出训练数据集中和测试样本最近的k个样本，“投票法”，由这k个近邻的中占最多的样本的类别来作为预测结果。

回归原理是，基于距离度量找到k个“邻居”之后，使用“平均法”，将k个样本的实值输出的平均值作为预测结果。

当k=1时，即类别为最近邻的实例的类别。


## 2.1 算法介绍

训练集中每个数据样本用一个n维特征向量描述n个属性的值，即 X = {x<sub>1</sub>，x<sub>2</sub>，...，x<sub>n</sub>}，共k个类别，分别用c<sub>1</sub>，c<sub>2</sub>，...,c<sub>k</sub>表示。训练数据集共N个样本，则
> 输入：
训练数据集 T={(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>),…,(x<sub>N</sub>,y<sub>N</sub>))} 
其中，x<sub>i</sub>∈X⊆R<sub>N</sub>为实例的特征向量，yi∈Y={c1,c2,…,cK} 为实例的类别，i=1,2,…,N ;新输入的实例表示为x；

> 输出：
 实例x所属的类y 

> 分类步骤：
根据给定的距离度量，在训练集T中找出与x最邻近的k个点，涵盖这k个点的x的领域记作N<sub>k</sub>(x),在N<sub>k</sub>(x)中根据分类决策规则（如多数表决）决定x的类别y: 

由算法原理和算法介绍知道k-近邻算法的三个基本要素：**k值的选择，距离度量和分类决策规则**。

## 2.2 算法步骤 

k-近邻算法的基本流程是：
![此处输入图片的描述][1]


1. `收集数据`：可以使用任何方法获取文本文件。 

2. `准备数据`：距离计算所需要的数值，最好是结构化的数据格式。
    1. 将数据解析处理为分类器可以接受的格式。
    2. 有可能其中一个特征值对计算结果的影响远远大于其他的特征值，而这是我们不希望的，我们更希望所有的特征是同等重要的，这时就需要一种方法将`数值归一化`，比如，将取值范围处理为0到1，或者-1到1之间。这样会增加算法复杂度，但可以得到准确结果，是很值得的。

3. `分析数据`：可以使用任何方法。 （训练算法，k-近邻不需要）
    这里的“无训练过程”需要理解的是：k-近邻算法是没有显式的训练过程的。要理解这句话就要了解“懒惰学习”：此类学习技术在训练阶段仅仅是把样本保存起来，训练时间开销为0,在得到测试样本后在进行处理。k-近邻是懒惰学习的代表，在训练阶段只是训练保存数据，没有真正的训练过程，直到得到测试样本，载进行距离度量等步骤。

    与“懒惰学习”相对应的是“急切学习”，在训练阶段就对样本进行学习处理。
    
4. `测试算法`：计算错误率。 
需要测试集，测试样本是已经分类的样本集，如果得到的目标变量和实际的类别不同，则标记为一个错误。

5. `应用阶段,使用算法`：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输入数据分别属于哪个分类。即距离度量和判别类别。

下面具体介绍距离度量方法和k值的选择。

# 三、k-近邻三要素

## 3.1 k值选择
k值得选择对结果的影响可以看一个简单的例子：
![k值选择][2]

如果K=3，绿色圆点的最近的3个邻居是2个红色小三角形和1个蓝色小正方形，“投票表决”，判定绿色的这个待分类点属于红色的三角形一类。

如果K=5，绿色圆点的最近的5个邻居是2个红色三角形和3个蓝色的正方形，判定绿色的这个待分类点属于蓝色的正方形一类。

所以，**k值得选择会对结果产生重要影响**。

若选择较小的k值，因为k越小，选出的样本会越接近输入的测试样本，这样，学习的近似误差会减小；但k越小，容错性越差，因为预测结果对近邻的实例点就会越敏感，若近邻的实例恰巧是噪声则预测往往会出错，这样学习的估计误差会增大。这样k值减少说明模型比较复杂，容易发生过拟合。

若选择较大的k值，即用较大邻域中的训练样本参与预测，学习的估计误差会减少，但学习的近似误差会增大，因为这时距离输入实例较远的训练样本也参与了测试样本的预测，使得预测发生错误。这样k值增大意味着模型变得简单。

当 $k=N$ 时所有的新输入实例都在同一类，此时模型最简单,这样过于简单的模型，丢失了训练样本中大量的有用信息。

应用中，k值回去一个比较小的数字，一般是不会大于20的整数，通常通过交叉验证法选取最优k值。

## 3.2 距离度量
在特征空间中，两个样本点之间的距离反映了两个样本点之间的相似程度，决定哪些样本是待分类样本的k个最近邻居，常用的距离度量方法是：欧氏距离和曼哈顿距离等。

### 欧几里德距离（欧氏距离）： 
欧式距离是最常见的衡量两个点之间距离的方法。

1. 二维平面上两点x(x1,y1)与y(x2,y2)间的欧氏距离：

$d(x,y) :=\sqrt{(x_1-y_1)^2 + (x_2-y_2)^2}$

2. 三维空间两x(x1,y1,z1)与y(x2,y2,z2)间的欧氏距离：

$d(x,y) :=\sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + (x_3-y_3)^2}$

3. 两个n维向量欧氏点 x = (x1,...,xn) 和 y = (y1,...,yn) 之间的距离：

$d(x,y) :=\sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 +...+ (x_n-y_n)^2} = $

$\sqrt{\sum_{i=1}^n|x^l_i - x^l_j|^2} $


### 曼哈顿距离

$d(x,y) := \sum_{i=1}^n|x^l_i - x^l_j|$


## 3.3 算法优缺点
`优点`：在训练样本数目大时获取很高的精度、对异常值不敏感、无数据输入假定。
`缺点`：计算复杂度高、空间复杂度高，训练集较小时泛化能力差，无法给出数据的内在含义，不能识别出哪个特征比其他特征更具识别力
`适用数据范围`：数值型和标称型

# 四、 实例
判断 CC 是属于哪一种类型的电影，下图是收集的数据：
![实例][3]

1. 确定特征属性

   我们确定特征属性为打斗镜头和接吻镜头2个特征。

2. 获取训练样本

    表统计数据

3. 确定 $k=3$
4. 计算距离
采用欧式距离计算，得到：
![此处输入图片的描述][4]

3个最近的电影是 California Man，He，Beautiful，都是爱情片，所以判定CC是爱情片。

  [1]: http://omxy7x542.bkt.clouddn.com/17-12-7/38044178.jpg
  [2]: http://omxy7x542.bkt.clouddn.com/17-12-7/66344788.jpg
  [3]: http://omxy7x542.bkt.clouddn.com/17-12-7/18743437.jpg
  [4]: http://omxy7x542.bkt.clouddn.com/17-12-7/75173719.jpg