# 机器学习：决策树算法学习笔记

标签（空格分隔）： 机器学习

---

# 一、应用背景

前面介绍了一种监督学习算法——k-近邻算法，k-近邻算法简单，可以完成很多分类和回归任务，但是“无法给出数据的内在含义，不能识别出哪个特征比其他特征更具识别力 ”，所以，这里介绍另一种监督学习算法——决策树算法。

决策树算法概念非常简单，也是最经常使用的数据挖掘算法，因为决策树是基于特征的切分，可以生成基于特征选择的不同预测结果的树状结构，当希望更好的理解手上的数据的时候往往可以使用决策树。如专家系统，以及其他更为广泛的机器学习领域。

理解决策树之前，先介绍一个游戏——“二十个问题”的游戏，游戏规则很简单：参与游戏的默想一个事物，其他游戏参与者对其进行提问，最多提问20个问题，回答也只能是对或错。问问题的人进行推断分解，逐步缩小待猜测事物的范围。这个游戏和**决策树**的原理类似，可以看做是一些列if-then规则的集合。

下面重点讨论贝叶斯方法的数学基础，算法步骤以及应用场景。




#二、决策树算法
决策树是一种常用的监督学习方法，主要用来解决分类与回归问题。决策树基于树结构进行决策，每一步决策的结果或是导出最终结果或是导出进一步的判定问题，而这个“进一步的判定”是在上一步决策结果的限定范围内的，如此逐渐构造一颗决策树。

决策树根据训练数据集，从中提取一些列规则，创建规则，就是决策树的机器学习过程。

那么决策树是怎么构成的，训练数据集是什么样的，测试样本是如何取得预测结果的，下面一一介绍。

## 2.1 决策树
 
下面以一个例子说明决策树的各种概念。
![数据表格][1]

下图是根据数据表格建立的一棵决策树，不考虑如何建立的，简单看决策树的构成。
![决策树][2]
一棵决策树首先是一棵树，其中包含一个根节点，若干内部节点，若干叶子节点。

决策树的思想就是简单直观的“分而治之” 的策略。决策树中每个节点都存储着一定量的数据集，根节点最多，包括训练数据的全集，叶节点对应于决策结果，其他节点的数据集对应于一个属性测试，根据测试的结果划分到子节点中。根节点到某一个子节点的那条路径，对应了一个判定测试序列。决策树学习的目的就是建立一颗泛化能力强，即处理未见实例数据强的决策树。

## 2.1 算法介绍

训练集中每个数据样本用一个n维特征向量描述n个属性的值，即 X = {x1，x2，...，xn}，共k个类别，分别用c1，c2，...,ck表示。训练数据集共N个样本，则
>输入：训练集D={(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>),…,(x<sub>m</sub>,y<sub>m</sub>))} 
    属性集 A = {a<sub>1</sub>,a<sub>2</sub>,...，a<sub>d</sub>}
    
> 输出：以node为根结点的一棵决策树 

过程：函数TreeGenerate(D,A)

```  
//生成根节点后，开始解析训练数据，首先考虑3种情况：（1）全部样本都属于同一类别，无需划分，直接返回类型标签。
//（2）属性集A为空，或者所有数据在所有属性值上的取值相同，同样无需划分，直接返回。这种情况下，直接返回类别标记为中样本数最多的类
//（3）当前节点包含的样本集合为空，将当前节点标记为叶节点，类别设定为父节点包含数据集中所含样本最多的类，返回。

生成结点node; 

if 中样本全属于同一类别C then 
    将node标记为类叶结点; return 
end if 
if A = $\emptyset$ OR  D中样本在属性集上上取值相同 then 
    将node标记为叶结点，其类别标记为中样本数最多的类; return 
end if 

//下面的步骤是第三种情况，决策树的生成是一个递归过程，这涉及了真正的算法步骤，在下面详细介绍每一步。

从中选择最优化分属性 a*，划分数据集，创建分支节点
for a*的每一个值a** do 
为node生成一个分支; 令Dv表示D中在a*上取值为a**的样本子集; 
    if Dv为空 then 
        将分支结点标记为叶结点，其类别标记为D中样本最多的类;return;
    else 
        以TreeGenerate(D,A\{a*})为分支结点 
    end if 
end for 
```

## 2.2 算法步骤

决策树算法的基本流程是：

![流程][3]

`收集数据`：可以使用任何方法。 
`准备数据`：**树构造算法只适用于标称数据，因此数值型数据必须离散化。** 
`分析数据`：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。 
`训练算法`：构造树的数据结构。 
`测试算法`：使用经验树计算错误率。 
`使用算法`：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。


这里看决策树的构造包括3个步骤：特征选择，决策树的生成和决策树的修剪。下面一一介绍。


# 三、 决策树算法

决策树算法主要包括3个步骤：特征选择，决策树的生成和决策树的修剪。
## 3.1 特征选择余 与 决策树生成
算法中的下面这一步很重要，选择最优划分属性。
```
从中选择最优化分属性 a*，划分数据集，创建分支节点
```
“最优”即最具有判别力的属性，即当前数据集上哪个特征在划分数据分类时起决定性作用。
构建决策树的``大原则：使得无序的数据变得更加有序``。所以，在构建决策树的过程中，最好的情况是，希望分支节点的样本尽可能属于同一个类别，即“纯度”越来越高。所以，特征选择就是在划分的时候希望得到纯度最高的划分，对纯度的不同计算方式，决定了不同的划分方式。

这一过程要找到决定性的特征，划分出最好的结果，必须评估每个特征。评估准则即“纯度”。要计算“纯度”，首先了解几个基本概念。

### 3.1.1 信息熵
信息熵是度量样本集合纯度最常用的方法，信息熵越大，越混乱，信息熵越小，越有序。要找纯度越高的，即找信息熵越小的。样本集合D的信息熵为：

$ Ent(D) =  - \sum_{k=1}^{|y|} p_klog_2p_k$

其中， $p_k$为当前样本集合D中第 $k$ 类样本所占的比例。

以上面的西瓜数据集为例，共17个数据集，正例 $p_1 = \frac{8}{17}$ ,反例$p_2 = \frac{9}{17}$ ,计算根节点的信息熵为：
$Ent(D) =  - \sum_{k=1}^{|y|} p_klog_2p_k $
$= -(\frac{8}{17}log_2\frac{8}{17} + \frac{9}{17}log_2\frac{9}{17}) = 0.998 $

以色泽为例，它的可能取值为{青绿，乌黑，浅白}，则根据色泽属性对D进行划分，则可得到3个子集，分别为$D_1$(色泽=青绿)，$D_2$(色泽=乌黑)，$D_3$(色泽=浅白)，其中$D_1$(色泽=青绿)共有6个样例，其中正例为$\frac{3}{6}$，反例为$\frac{3}{6}$；$D_2$(色泽=乌黑)共有6个样例，其中正例为$\frac{4}{6}$，反例为$\frac{2}{6}$；$D_3$(色泽=青绿)共有5个样例，其中正例为$\frac{1}{5}$，反例为$\frac{4}{5}$.则所获得3个分支节点的信息熵为：

$Ent(D_1) =  -(\frac{3}{6}log_2\frac{3}{6} + \frac{3}{6}log_2\frac{3}{6}) = 1.000 $

$Ent(D_2) =  -(\frac{4}{6}log_2\frac{4}{6} + \frac{2}{6}log_2\frac{2}{6}) = 0.918 $

$Ent(D_3) =  -(\frac{1}{5}log_2\frac{1}{5} + \frac{4}{5}log_2\frac{4}{5}) = 0.722 $

假定离散属性a有V个可能的取值a1,a2,...,aV,若使用a来对样本集D进行划分，则会产生V个分支节点，其中第v个分支节点包含了D中所有在属性a上取值为av的样本，记为Dv。

单纯信息熵并不能很好的计算“纯度”，考虑到不同的分支节点所包含的样本数不同，给分支节点赋予权重 $|D_v|/|D|$ ,即样本数越多的分支节点的影响越大，于是可计算出用属性a对样本集D进行划分所获得的”信息增益”。


### 3.1.2 信息增益

$ Gain(D,a) =  Ent(D) - \sum_{v=1}^{V}\frac{|D_v|}{|D|}Ent(D^v)$

一般而言，信息增益越大，则意味着使用属性a来进行划分所得的”纯度提升”越大。因此，我们可以用信息增益来进行决策树的划分属性选择。
信息增益Gain(D,色泽)应该为：
$Gain(D,色泽) = Ent(D) - \sum_{v=1}^{V}\frac{|D_v|}{|D|}Ent(D^v) $
$ = 0.998-(\frac{6}{17}* 1.000 + \frac{6}{17}* 0.918) + \frac{5}{17}* 0.722)$
$ = 0.109 $

类似的我们可以得到其他属性的信息增益，如下：

$ Gain(D,根蒂) = 0.143；Gain(D,敲声) = 0.141；$

$ Gain(D,纹理) = 0.381；Gain(D,脐部) = 0.289；$

$ Gain(D,触感) = 0.006 $
显然，属性纹理的信息增益最大，于是它被作为划分属性。
然后决策树学习算法将对每个分支节点做进一步划分，以第一个分支节点清晰为例，该节点共有9个样例，此时可选属性为{色泽，根蒂，敲声，脐部，触感},基于$D_1$计算各属性的信息增益：

$ Gain(D1,色泽) = 0.043；Gain(D1,根蒂) = 0.458；$
$ Gain(D1,敲声) = 0.331；Gain(D1,脐部) = 0.458；$
$Gain(D1,触感) = 0.458；$

其中有3个属性均取得了最大的信息增益，可任选其中之一作为划分熟悉，类似的，对每个分支节点进行上述操作，最终得到以下的决策树。
![此处输入图片的描述][4]


但是这种方式也有缺陷：对可取值数目较多的属性有所偏好，所以，一般不用信息增益划分。

或者也可以使用`	增益率`   和   `基尼系数`



## 3.2 决策树的修剪
剪枝处理是决策树学习算法对付过拟合的主要手段，通过去掉一些分支来降低过拟合的风险。
 有两种不同的剪枝方式：预剪枝，后剪枝
 
###3.2.1预剪枝

预剪枝的本质是“贪心”，在决策树生成过程中，对每个节点在划分前先进行预估，若当前节点的划分不能带来决策树泛化性能的提升，则停止划分并将当前节点标记为叶节点。
这种处理方式有效降低了过拟合的风险，并且减少了决策树的训练时间和测试时间，但这种“贪心”的做法，有可能造成“欠拟合”的风险。

### 3.2.2后剪枝

先从训练集中生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对应的子树替换成叶结点能带来决策树泛化性能的提升，则将该子树替换成叶节点。 
这样生成决策树之后再剪枝的做法，需要建立决策树之后再自底向上对书中的所有非叶节点逐一判断，训练开销比未剪枝和预剪枝大得多。好处是欠拟合风险较小泛化性能高于预剪枝。

## 3.3 算法优缺点

`优点`：计算复杂度不高，分类快，只需要构建一次，之后的判断可以重复使用，每一次预测的最大计算次数不超过决策树的深度。输出结果具有可读性，易于理解，有助于人工分析，对中间值的缺失不敏感，可以处理不相关特征数据。
`缺点`：可能会产生过度匹配问题，即过拟合



  [1]: http://omxy7x542.bkt.clouddn.com/17-12-8/77149312.jpg
  [2]: http://omxy7x542.bkt.clouddn.com/17-12-8/69700679.jpg
  [3]: http://omxy7x542.bkt.clouddn.com/17-12-8/27751437.jpg
  [4]: http://omxy7x542.bkt.clouddn.com/17-12-8/69700679.jpg