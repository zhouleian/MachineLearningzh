# 机器学习：误差逆传播算法学习笔记

标签（空格分隔）： 机器学习

---

# 一、应用背景
在《机器学习：感知机算法学习笔记》一文中，介绍了神经网络的基本概念和数学接触、感知机模型，感知机模型是一种只有两层网络的（输入层，输出层）简单神经网络，用来实现二分类任务。
感知机算法简单易于实现，但是只有一层输出层是功能神经元（进行激活函数的处理），对于非线性可分的问题就无法解决了，所以学习能力非常有限。

所以，要解决非线性可分的问题，就需要使用多层功能神经元，这时候，隐层和输出层都是具有激活函数的功能神经元。

神经网络学习隐藏在阈值和权重中，所以，多层功能神经元的学习能力比单层感知机强得多，这时，感知机的简单规则就不够用了，需要更强大的算法训练多层神经网络，误差逆传播是迄今最成功的多层神经网络训练算法，当在任务中使用神经网络时大多是用神经网络来训练的。

前面介绍了神经元，神经网络的基本概念和数学基础，下面主要介绍误差逆传播的基本概念和算法实现。

# 二、误差逆传播（ error BackPropagation，简称BP）基本概念

BP网络是一种多层神经网络，首先介绍神经网络中的多层前馈神经网络是什么，然后依次介绍BP算法的一些基本概念。

## 2.1 多层前馈神经网络（multi-layer feedforward neural networks）
多层前馈神经网络中每层神经元与下一层神经元完全互连，神经元之间不存在同层连接，也不存在跨层连接。使用激活函数来描述层与层输出之间的关系，从而模拟各层神经元之间的交互反应。

其中，“前馈”：网络拓扑结构上不存在环或回路。 BP算法不仅用于多层前馈神经网络的学习，还可用于其他类型网络学习，比如，训练递归神经网络，但通常BP算法指的是多层前馈神经网络。
![多层前馈][1]

下面以一个最简单的3层 BP 神经网络来介绍BP算法:
![BP][2]

## 2.2 梯度下降算法
梯度上升算法用来求函数的最大值，梯度下降算法用来求函数的最小值。
要寻找目标函数曲线的波谷，这个方向可以通过微分得到。

梯度下降法的思想是：要找到某个函数的最小值，即要找到目标函数的波谷，采用贪心法：想象一个小人站在半山腰，他朝哪个方向跨一步，可以使他距离谷底更近（位置更低），就朝这个方向前进。这个方向的可以通过微分得到，即最好的方法是沿着函数的梯度方向寻找（函数值下降最快的方向）。

解决了最小值的移动方向问题，移动量也很重要，移动量成为步长，步长称为$\alpha$。则梯度下降算法的迭代公式为：

$$ w:= w - \alpha\nabla_wf(w) $$

梯度上升算法是：

$$ w:= w + \alpha\nabla_wf(w) $$

这个公式一致迭代执行，直到达到某个停止条件为止。

此时，还涉及$\alpha$的初始值的问题，步子太小的话，速度太慢；过大的话，容易发生抖动，可能到不了谷底。

显然，距离谷底较远（位置较高）时，步幅大些比较好；接近谷底时，步幅小些比较好（以免跨过界）。距离谷底的远近可以通过梯度（斜率）的数值大小间接反映，接近谷底时，坡度会减小。所以，我们希望步幅与梯度数值大小正相关。

## 2.3 反向
误差逆传播这里的“逆”即反向，首先，正向传播是指：输入样本 ——>输入层 ——> 隐藏层 ——> 输出层。
反向即将误差从隐藏层传递到输入层，动态改变权值。
BP算法实现围绕“信号的正向传播——>误差的逆向传播”的过程。

# 三、BP算法
BP算法是多层神经网络训练算法，将输入实例提供给输入层神经元， 逐层传播信号，计算输出层误差，基于某种策略，将误差反向通过隐藏层传递到输入层，对网络中的权值进行进行自动调整，直到网络输出的误差减小到可接受的程度或者进行了事先预定的学习次数为止。最终学习得到一个模型，对应给定输入给出我们期望的输出。

所以，BP算法总结即
`传播目的`:最终学习得到一个模型，对应给定输入给出我们期望的输出。
`传播对象`:误差
`逆传播`:逆传播即“后向传播”.
`算法本质`:对各连接权值的动态调整
`算法学习的方式`：在外界输入样本的刺激下不断改变网络的连接权值

下面介绍BP算法的具体实现：
![此处输入图片的描述][3]
## 3.1 算法介绍
训练集 $ D = \lbrace(\vec{x_1},\vec{y_1}),(\vec{x_2},\vec{y_2}),...,(\vec{x_m},\vec{y_m})\rbrace $ 
输入层有n个神经元，隐藏层有 $l$ 个神经元，输出是 $l$ 维实值向量，$q$个隐藏层神经元。

`阈值`：输出层第$j$个神经元的阈值为 $\theta_i$,隐层第h个神经元的阈值为$\gamma_h$。

`权重`：输入层第$i$个神经元与隐层第 $h$ 个神经元之间的权重 $v_{ih}$ ，隐层第 $h$ 个神经元与输出层第 $j$ 个神经元之间的权重  $w_{hj}$

图中指明了第j个输出神经元的输入和第h个隐层神经元的输入，其中 $x_i$ 为输入， $b_h$ 为隐层第h个神经元的输出。

BP算法任意参数$v$的更新估计式为：
$$ v<—  v+\Delta{v} $$
BP使用递归下降策略，以目标的负梯度方向对参数进行调整。
给定学习率，算法中需要更新的参数为：
 $\Delta{w_{hj} }$
 $\Delta{theta_j} $
 $\Delta{v_{ih}}$ 
 $\Delta{\gamma_h}$
这些需要推导，西瓜书上讲的很明白。

学习率控制每一步迭代中的更新步长，对于学习率，太小的话，收敛速度太慢；过大的话，容易发生抖动，可能到不了谷底。


## 3.2 算法步骤

给定计算精度 $\epsilon$ 和最大学习次数 M。
>输入：
    训练集 :$ D = \lbrace(\vec{x}_k,\vec{y}_k)^m_{k=1}\rbrace$
    学习率: $\eta$ ;
    
$ $
过程：
>在(0, 1)范围内随机初始化网络中所有连接权和阈值，
repeat:
    for all $(\vec{x_k},\vec{y_k})\in D$ do 
        计算隐藏层各个神经元的输入输出
　　    根据当前参数计算当前样本的输出$\hat{y}_k$;
　　    计算输出层神经元的梯度项$g_j$；
　　    计算隐层神经元的梯度项$e_h$；
　　    根据更新公式，更新连接权${w_{hj} }$ 、 ${v_{ih}}$ 与阈值 $ \theta_j、\gamma_h$,
　　end for
until 达到停止条件

停止条件：误差达到了设定的精度或者，学习次数超过了设定的最大次数，则结束。

>输出：连接权与阈值确定的多层前馈神经网络







  [1]: http://omxy7x542.bkt.clouddn.com/17-12-13/12591343.jpg
  [2]: http://omxy7x542.bkt.clouddn.com/17-12-13/95888074.jpg
  [3]: http://omxy7x542.bkt.clouddn.com/17-12-13/95888074.jpg