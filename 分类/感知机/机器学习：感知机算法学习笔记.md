# 机器学习：感知机算法学习笔记

标签（空格分隔）： 机器学习

---

# 一、应用背景

在机器学习中分类是一个非常常见的任务之一，计算机程序需要指定哪些分类是 $k$ 类中的哪一类。当 $k = 2$ 时，即为非常典型的二分类任务，感知机即为一种二分类的线性分类模型，由Rosenblatt于1957年提出的，是神经网络和支持向量机的基础。

感知机是一种人工神经网络，下面从神经元开始逐步介绍感知机的神经网络和数学基础，感知机算法是如何学习的，以及算法步骤。

# 二、 感知机基本概念

## 2.1 神经元模型

神经网络在机器学习中经常出现，现在已经是一个相当庞大、多学科交叉的学科领域。这里仅仅为说明感知机做简单介绍。神经网络的定义很多，使用最广泛的一种是：神经网络是由具有适应性的简单单元组成的广泛互联的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的交互反应。

神经网络的基本单元就是“神经元模型”，它的设计灵感来源于生物学上神经元的信息传播机制。

首先介绍生物学上的神经元。

### 2.1.1 生物神经元
下图为生物学上的神经元简单结构示意图：

![此处输入图片的描述][1]
$$图1$$
可以看到神经元包括胞体和突起两部分，突起是神经元胞体的延伸部分，由于形态结构和功能的不同，可分为树突和轴突。

树突是从胞体发出的一个至多个突起呈放射状。靠近胞体的部分要粗，经过反复分支而变细，形如树枝状。树突具有接受刺激并将冲动传入细胞体的功能； 轴突是一根长神经纤维，其主要功能是将神经冲动由胞体传至其他神经元。轴突传导神经冲动的起始部位是在轴突的起始段，沿轴模进行传导。每根神经元只有一根轴突。

突触：神经元与神经元之间的连接点称为突触。它是神经元之间的传递信息的关键性结构。突触可以分为两类，即化学性突触和电突触。

每一个神经元与其他神经元通过在生物神经网络中神经元存在“兴奋”和“抑制”两种状态。一般情况下，大多数的神经元是处于抑制状态，但是一旦某个神经元收到刺激，导致它的电位超过一个阈值，那么这个神经元就会被激活，处于“兴奋”状态，向其他的神经元传播化学物质（其实就是信息）。当它兴奋时，就会想相连的神经元发送化学物质，从而改变这些神经元内的电位。
　　
### 2.1.2 神经网络神经元
　　
神经网络的单位是神经元，与生物神经元相似，当他兴奋的时候（超过一定的阈值），便会与相邻的神经元发送信息。这就是“M-P神经元模型”：

![此处输入图片的描述][2]
$$图 2$$
在这个模型中，神经元接收来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，把总输入与阈值进行比较，通过“激活函数”处理产生神经元的输出。


## 2.2 激活函数

神经元最终的输出是经过“激活函数”的处理的，激活函数的发展，是近年来，深度学习在计算机视觉领域取得了引人注目的成果的其中一个重要因素。这里仅仅介绍激活函数定义，和与感知机有关的激活函数，详细的讲解在另一篇博客中。


### 2.2.1 激活函数的定义

在神经网络中，神经元的输出都是经过激活函数处理后再输出的。即神经元节点的激活函数定义了对神经元输出的映射。例如，在图2的神经元模型中，神经元的最终的输出 $y$ 就是经过激活函数 $y$ 的处理。

因此激活函数的定义：激活函数是映射 $ h:R→R$，且几乎处处可导。

### 2.2.2 激活函数的历史发展与近期进展
从定义来看，几乎所有的连续可导函数都可以用作激活函数。但目前常见的多是分段线性和具有指数形状的非线性函数。在感知机中主要使用的是 Sigmoid 函数作为激活函数

### 2.2.3 Sigmoid函数

感知机是一个二分类的线性分类模型，在二分类任务中，最终的输出是1 和 0，最理想的函数是阶跃函数：
![此处输入图片的描述][3]
$$图 3 $$
阶跃函数，虽然用“1”代表神经元兴奋，“0”代表神经元的抑制，但是，因为阶跃函数具有的不连续，不光滑等不太好的性质，所以常用Sigmoid函数作为激活函数，Sigmoid可以将函数的输出压缩在（0,1）范围内：
![此处输入图片的描述][4]
$$图 4 $$

结合神经元和激活函数，将这样的多个神经元按照一定的层次结构连接起来，就得到了神经网络。感知机是一个仅有两层神经元的简单神经网络。下面介绍感知机模型。

# 三、感知机算法



## 3.2 感知机模型
感知机由两层神经元组成：输入层、输出层，输入层接收外界输入信号后传递给输出层，输出层是M-P神经元，亦称“阈值逻辑单元”。神经网络能学到的东西蕴含在连接权和阈值中。 
![此处输入图片的描述][5]


## 3.2 算法介绍
假设输入空间(特征向量)为X⊆Rn，输出空间为Y={-1, +1}。
    
>输入:x∈X表示实例的特征向量，对应于输入空间的点.
输出:y∈Y表示示例的类别。

由输入空间到输出空间的函数为

  $  f(x)=sign(w·x + b) $
  
称为感知机。其中，参数w叫做权值向量，b称为偏置。w·x表示w和x的内积。sign为符号函数，即

$$ sign(x)= \begin{cases} +1, & \text {x>=0} \\ -1, & \text{x<0} \end{cases} $$

## 3.2 算法步骤

感知机其实是一种最优化问题：给定数据集T={(x1,y1),(x2,y2)...(xN,yN)}（其中xi∈X=Rn，yi∈Y={-1, +1}，i=1,2...N），求参数w,b,使其成为损失函数的解（M为误分类的集合）：

$$ min_{w,b}L(w,b) = - (\sum_{{x_i}{\inM}}y_i(w*x_i)+b)$$

感知机算法的学习是误分类驱动的，采用的是随机梯度下降法。首先，任意选定$w_0,b_0$，然后用梯度下降法不断极小化目标函数,极小化的过程不知一次性的把M中的所有误分类点梯度下降，而是一次随机选取一个误分类点使其梯度下降。

```
输入：T={(x1,y1),(x2,y2)...(xN,yN)}（其中xi∈X=Rn，yi∈Y={-1, +1}，i=1,2...N，学习速率为η）
输出：w, b;感知机模型f(x)=sign(w·x+b)
(1) 初始化w0,b0
(2) 在训练数据集中选取（xi, yi）
(3) 如果yi(w xi+b)≤0
           w = w + ηyixi
           b = b + ηyi
(4) 转至（2）
```
`特点`：两层神经元，意味着感知机只有一层输出层是功能神经元（进行激活函数的处理），感知机能容易的实现逻辑与、或、非运算，因为这些都是线性可分的（即可以用一个超平面分开），但对于非线性可分的问题就无法解决了，所以学习能力非常有限。
所以，要解决非线性可分的问题，就需要使用多层功能神经元，这时候，隐层和输出层都是具有激活函数的功能神经元。

# 四、实例
对于训练数据集，其中正例点是x1=(3,3)T,x2=(4,3)T，负例点为x3=(1,1)T，用感知机学习算法的原始形式求感知机模型f(x)=w·x+b。这里w=(w(1),w(2))T，x=(x(1),x(2))T

解：构建最优化问题：$ min_{w,b}L(w,b) = - (\sum_{{x_i}{\inM}}y_i(w*x_i)+b)$
按照算法求解w， b。η=1

(1)取初值w0=0, b0=0

(2)对于（3，3）:-(0+0)+0=0未被正确分类。更新w,b
w1=w0+1*y1·x1 = (0,0)T+1(3,3)T=(3,3)T
b1=b0+y1=1
得到线性模型w1x+b1 = 3x(1)+3x(2)+1

(3)返回（2）继续寻找yi(w·xi+b)≤0的点，更新w,b。直到对于所有的点yi(w·xi+b)>0，没有误分类点，损失函数达到最小。
分离超平面为x(1)+x(2)-3=0
感知机模型为 f(x)=sign(x(1)+x(2)-3)

  [1]: http://omxy7x542.bkt.clouddn.com/17-12-9/78514768.jpg
  [2]: http://omxy7x542.bkt.clouddn.com/17-12-9/74655035.jpg
  [3]: http://omxy7x542.bkt.clouddn.com/17-12-9/55732325.jpg
  [4]: http://omxy7x542.bkt.clouddn.com/17-12-9/50727929.jpg
  [5]: http://omxy7x542.bkt.clouddn.com/17-12-9/37407583.jpg